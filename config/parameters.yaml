# Training hyperparameters (shared across all experiments)
batch_size: 64
epochs: 100
weight_decay: 0.0001
momentum: 0.9

# Optimizer-specific learning rates
learning_rates:
  adam: 0.001
  adamw: 0.001
  sgd: 0.01

# Scheduler settings (shared across all experiments)
scheduler: "step"
scheduler_step_size: 30
scheduler_gamma: 0.5

# Validation
val_split: 0.2
early_stopping_patience: 10
